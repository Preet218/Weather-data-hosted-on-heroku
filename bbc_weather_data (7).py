# -*- coding: utf-8 -*-
"""BBC weather data

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q1I_NvjCdGhKxmjAL1TMiI4hSOaSpxMY

# BBC Weather Data 
In this project we obtain the weather prediction data of any city using BBC weather website. 
First, we make an api call to obtain the location id of the city we want the weather data. Next using the location id we make connection with the bbc database to extract high, low and summary of the day's temperature for the next 14 days.
"""

# pip install streamlit

from urllib.parse import urlencode    
import requests       # To make connection with the webpage
from bs4 import BeautifulSoup as bs      # To parse the webpage
import pandas as pd                      
import datetime    
import streamlit as st

st.header("Enter city name")

city = st.text_input("City Name")     # City name 
location_url = 'https://locator-service.api.bbci.co.uk/locations?' +urlencode({   
      'api_key' : 'AGbFAKx58hyjQScCXIYrxuEwJh2W2cmv' ,
      'stack' : 'aws' ,
      'locale' : 'en', 
      'filter' : 'international', 
      'place-types' : 'settlement,airport,district', 
      'order' : 'importance', 
      's' : city, 
      'a' : 'true', 
      'format' : 'json'
})

result = requests.get(location_url).json()    # Making an API call to get the location id of the city

try:
  city_id = result['response']['results']['results'][0]['id']     # Extracting the location id

  url = 'https://www.bbc.com/weather/' + city_id        
  data = requests.get(url)      # Making connection with the database with location id
  soup = bs(data.content, 'html.parser')    # Using Beautiful soup to parse the webpage
  high_temp = soup.find_all('span', 'wr-day-temperature__high-value')     # Getting high temperature in celsius and fehrehite for 14 days
  low_temp = soup.find_all('span', 'wr-day-temperature__low-value')       # Getting low temperature in celsius and fehrehite for 14 days
  weather_summary = soup.find_all('div', 'wr-day__details__weather-type-description')   # Getting summary for 14 days

  high_c = [int(x.text.split()[0][:-1]) for x in high_temp]    # Extracting only the celsius temperature
  low_c = [int(x.text.split()[0][:-1]) for x in low_temp]
  summary = [x.text for x in weather_summary] 

  if len(high_c) != len(low_c):
      high_c = [low_c[0]] + high_c  # this is only when the high value of the day is not-available

  date = [(datetime.date.today() + datetime.timedelta(days=i)).strftime('%d-%m-%Y') for i in range(14)]   # List of date for next 14 days

  weather_df = pd.DataFrame({'Date' : date, 'High' : high_c, 'Low' : low_c, 'Summary' : summary})   # Creating pandas dataframe from the lists
  weather_df.head(15)
  weather_df.style.set_caption(city + ' Weather Prediction for next 14 days')

  r = "Weather data of " + city + " for the next 14 days"
  st.write(r)
  st.write(weather_df)

except:
  st.write("Please enter a valid city name")

# from urllib.parse import urlencode    
# import requests       # To make connection with the webpage
# from bs4 import BeautifulSoup as bs      # To parse the webpage
# import pandas as pd                      
# import datetime    
# import streamlit as st

# st.header("Enter city name")

# city = st.text_input("City Name")     # City name 
# location_url = 'https://locator-service.api.bbci.co.uk/locations?' +urlencode({   
#       'api_key' : 'AGbFAKx58hyjQScCXIYrxuEwJh2W2cmv' ,
#       'stack' : 'aws' ,
#       'locale' : 'en', 
#       'filter' : 'international', 
#       'place-types' : 'settlement,airport,district', 
#       'order' : 'importance', 
#       's' : city, 
#       'a' : 'true', 
#       'format' : 'json'
# })

# result = requests.get(location_url).json()    # Making an API call to get the location id of the city
# result

# city_id = result['response']['results']['results'][0]['id']     # Extracting the location id

# Getting Data based on city id

# url = 'https://www.bbc.com/weather/' + city_id        
# data = requests.get(url)      # Making connection with the database with location id
# soup = bs(data.content, 'html.parser')    # Using Beautiful soup to parse the webpage
# high_temp = soup.find_all('span', 'wr-day-temperature__high-value')     # Getting high temperature in celsius and fehrehite for 14 days
# low_temp = soup.find_all('span', 'wr-day-temperature__low-value')       # Getting low temperature in celsius and fehrehite for 14 days
# weather_summary = soup.find_all('div', 'wr-day__details__weather-type-description')   # Getting summary for 14 days

# high_c = [int(x.text.split()[0][:-1]) for x in high_temp]    # Extracting only the celsius temperature
# low_c = [int(x.text.split()[0][:-1]) for x in low_temp]
# summary = [x.text for x in weather_summary]

# if len(high_c) != len(low_c):
#     high_c = [low_c[0]] + high_c  # this is only when the high value of the day is not-available

# date = [(datetime.date.today() + datetime.timedelta(days=i)).strftime('%d-%m-%Y') for i in range(14)]   # List of date for next 14 days

# weather_df = pd.DataFrame({'Date' : date, 'High' : high_c, 'Low' : low_c, 'Summary' : summary})   # Creating pandas dataframe from the lists
# weather_df.head(15)
# weather_df.style.set_caption(city + ' Weather Prediction for next 14 days')

# from bs4 import BeautifulSoup as bs
# import requests
# import pandas as pd

# #Load the webpage
# r = requests.get("https://www.cricbuzz.com/cricket-match/live-scores")
# # Convert to a beautiful soup object
# soup = bs(r.content)
# # Print out HTML
# contents = soup.prettify()
# print(contents)

# r = "Weather data of " + city + " for the next 14 days"
# st.write(r)
# st.write(weather_df)